\documentclass[nojss]{jss}
\usepackage{enumitem}
\usepackage{float}
\usepackage{natbib} 
\usepackage[utf8]{inputenc}
\usepackage{listings}
\lstset{
  literate={ö}{{\"o}}1 
           {ä}{{\"a}}1
           {ü}{{\"u}}1
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Guillermo Vinu\'e\\
{\small Department of Statistics and O.R., University of Valencia, Valencia, Spain.}}
\title{\pkg{Anthropometry}: An \proglang{R} Package for Analysis of Anthropometric Data}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Guillermo Vinu\'e} %% comma-separated
\Plaintitle{Anthropometry: An R package for Analysis of Anthropometric Data} %% without formatting
\Shorttitle{Anthropometry: An R Package for Analysis of Anthropometric Data} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
The development of new powerful 3D scanning techniques has enabled the generation of broad updated anthropometric databases which constitute high valued data to improve the ergonomic design of products adapted to the user population. Accordingly, Ergonomics and Anthropometry are two fields more and more quantitative, so advanced statistical methodologies and modern software tools are demanded to get full benefit from anthropometric data. 

This paper presents a new \proglang{R} package, called \pkg{Anthropometry}, that brings together some statistical methodologies concerning clustering, statistical shape analysis, statistical archetypal analysis and the statistical concept of data depth, especially developed to deal with anthropometric data. They are proposed with the goal of providing effective solutions to some common anthropometric problems, such as clothing design or workstations design (focusing on the particular case of aircraft cockpits). The utility of the package is shown by analyzing the anthropometric data obtained from a survey of the Spanish female population performed in 2006 and from the 1967 United States Air Force Survey.
}

\Keywords{\proglang{R}, anthropometric data, clustering, statistical shape analysis, archetypal analysis, data depth}
\Plainkeywords{R, anthropometric data, clustering, statistical shape analysis, archetypal analysis, data depth}

\Address{
  Guillermo Vinu\'e\\
  Department of Statistics and Operations Research\\
  Faculty of Mathematics\\
  University of Valencia\\
  46100 Burjassot, Spain\\
  E-mail: \email{Guillermo.Vinue@uv.es}\\
  URL: \url{http://www.uv.es/vivigui}
}

%\usepackage{Sweave} %% already provided by jss.cls
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Developing statistical methodologies for Anthropometry}
%\VignetteDepends{Anthropometry}
%\VignetteKeywords{Anthropometric data, Clustering, Statistical shape analysis, Archetypal analysis, R}
%\VignettePackage{Anthropometry}


%The typical JSS paper will have a section explaining the statistical technique, a section explaining the code, a section with the actual code, and a section with examples. All sections will be made browsable as well as downloadable. The papers and code should be accessible to a broad community of practitioners, teachers, and researchers in the field of statistics.

\begin{document}

\section{Introduction}

Ergonomics is the science that investigates the interactions between human beings and the elements of a system. The application of ergonomic knowledge in multiple areas such as clothing and footwear design or both working and household environments is required to achieve the best possible match between the product and its users. To that end, it is fundamental to know the anthropometric dimensions of the targeted population. Anthropometry refers to the study of the measurements and dimensions of the human body and it is considered a very important branch of Ergonomics because its significant influence on the ergonomic design of products \citep{Pheasant2003}.

A major issue when developing new patterns and products that fit well the target population is the lack of up-to-date anthropometric data. Improvements in health care, nutrition and living conditions and the transition to a sedentary life style have changed the body dimensions of people over recent decades. The anthropometric databases must therefore be updated regularly. Traditionally, human physical characteristics and measurements have been manually taken by using rudimentary methods like calipers, rulers or measuring tapes \citep{Simmons2003,Lu2008,Shu2011}. These procedures are simple (user-friendly), non-invasive and no particularly expensive. However, measuring by hand a statistically useful sampling of thousands of people is time-consuming and error-prone: the set of measurements obtained and therefore the shape information is usually imprecise and inaccurate. 

In recent years, the development of new three-dimensional (3D) body scanner measurement systems has represented a huge step forward in the way of collecting and updating anthropometric data. This technology provides highly detailed, accurate and reproducible anthropometric data from which 3D shape images of the people being measured can be obtained \citep{Istook2001,Lerch2007,Wang2007,DApuzzo2009}. The great potential of 3D body scanning techniques constitute a true breakthrough in realistically characterizing people and they have made it possible to conduct new anthropometric large-scale size surveys in different countries (for instance, in the USA, the UK, France, Germany and Australia). In this context, the Spanish Ministry of Health sponsored a 3D anthropometric study of the Spanish female population in 2006 \citep{Alemany2010}. A sample of 10415 Spanish females from 12 to 70 years old, randomly selected from the official Postcode Address File, was measured. Associated software provided by the scanner manufacturers made a triangulation deriving the 3D spatial location of a large number of points on the body surface. A 3D binary image of the trunk of each woman (white pixel if it belongs to the body, otherwise black) is produced from the collection of points located on the surface of each woman scanned, as explained in \cite{icpram}. The two main goals of this study, which was conducted by the Biomechanics Institute of Valencia, were as follows: firstly, to characterize the morphology of females in Spain in order to develop a standard sizing system for the garment industry and secondly, to encourage an image of healthy beauty in society by means of mannequins that are representative of the population. In order to tackle both objectives, Statistics plays an essential role. 

In every methodological and practical anthropometric problem, body size variability within the user population is characterized by means of a limited number of anthropometric cases. This is what is called \emph{a user-centered design process}. An anthropometric case represents the set of body measurements the product evaluator plan to accommodate in design \citep{guidelines}. A case may be a particular human being or a combination of measurements. Depending on the features and needs of the product being designed, three types of cases can be distinguished: central, boundary and distributed. If the product being designed is a one-size product (one-size to accommodate people within a predetermined portion of the population) such as working environments design, the cases are selected on an accommodation boundary. However, if we focus on a multiple-size product (n sizes to fit n groups of people within a predetermined portion of the population), being clothing design the most apparent example, central cases are selected. The statistical methodologies that we have developed seek to define central and boundary cases to tackle the clothing sizing system design problem and the workplaces design problem (focusing on the particular case of an aircraft cockpit).

Clothing sizing systems divide a population into homogeneous subgroups based on some key anthropometric dimensions (size groups), in such a way that all individuals in a size group can wear the same garment \citep{libroAshdown,Chung2007}. An efficient and optimal sizing system must accommodate as large a percentage of the population as possible, in as few sizes as possible, that better describes the shape variability of the population. In addition, the garment fit for accommodated individuals must be as good as possible. Each clothing size is defined from a person who is near the center for the dimensions considered in the analysis. This central individual, which is considered as the size representative (the size prototype), becomes the basic pattern from which the clothing line in the same size is designed. Once a particular garment has been designed, fashion designers and clothing manufacturers hire fit models to test and assess the size specifications of their clothing before the production phase. Fit models have the appropriate body dimensions selected by each company to define the proportional relationships needed to achieve the fit the company has determined \citep{Ashdown2005,Workman2000,Workman1991}. Fit models are usually people with central measurements in each body dimension. The definition of an efficient sizing system depend to a large extent on the accuracy and representativeness of the fit models.

Clustering is the statistical tool that classifies a set of individuals in groups (clusters), in such a way that subjects in the same cluster are more similar (in some sense) to each other than to those in other clusters \citep{Kaufman90}. In addition, clusters are represented by means of a representative central observation. Therefore, clustering comes up naturally as a useful statistical approach to try to define an efficient sizing system and to elicite prototypes and fit models. Specifically, five of the methodologies that we have developed are based on different clustering methods. Four of them are aimed at segmenting the population into optimal size groups. The first one, hereafter referred to as \emph{trimowa}, has been published in \cite{Ibanez2012}. It is based on using a especial distance function that mathematically captures the idea of garment fit. The second and third ones (called \emph{CCbiclustAnthropo} and \emph{TDDclust} belong to a paper in progress \citep{VinueIbanez2013}. The \emph{CCbiclustAnthropo} methodology adapts to the field of Anthropometry a particular clustering algorithm proposed for the analysis of gene expression data. Besides, \emph{TDDclust} uses the statistical concept of data depth \citep{Liu1999} to group observations according to the most central (deep) one in each cluster. As mentioned, traditional sizing systems are based on using a suitable set of key body dimensions, so clustering must be carried out in the Euclidean space. In the three previous procedures, we have always worked in this way. Instead, in the fourth and last one, hereinafter called as \emph{kmeansProcrustes}, a clustering procedure is developed for grouping women according to their body shape, represented by a configuration matrix of anatomical markers (landmarks). To that end, the statistical shape analysis \citep{DrydenMardia1998} will be fundamental. This contribution has been submitted for publication \citep{Vinue2013ssa}. The fifth clustering proposal is presented with the goal of identifying accurate fit models. It is based on another clustering method originally developed for biological data analysis. This method, called \emph{hipamAnthropom}, has been published in \cite{Vinue2013}. From well-defined fit models and prototypes, representative and precise mannequins of the population can be made.

A sizing system is intended only to cover what is called ``standard'' population, leaving out the individuals who might be considered outliers respect to a set of measurements. In this case, outliers are called disaccommodated individuals. Clothing industries usually design garments for the standard sizes in order to optimize market share. The four aforementioned methods concerned with apparel sizing system design (\emph{trimowa}, \emph{CCbiclustAnthropo}, \emph{TDDclust} and \emph{kmeansProcrustes}) take into account this fact. In addition, because \emph{hipamAnthropom} is based on hierarchical features, it is able to discover and return true outliers. 

Unlike clothing design, where representative cases correspond to central individuals, in designing a one-size
product, such as working environments or the passenger compartment of any vehicle including aircraft cockpits, the most common approach is to search for boundary cases. In these situations, the variability of human shape is described by extreme individuals, which are those that have the smallest or largest values (or extreme combinations) in the dimensions considered in the study. These design problems fall into a more general category: the accommodation problem. The supposition is that the accommodation of boundaries will facilitate the accommodation of interior points (with less-extreme dimensions) \citep{Bertilsson2012,Parkinson2006,guidelines}. For instance, a garage entrance must be designed for a maximum case, while for reaching things such as a brake pedal, the individual minimum must be obtained. In order to tackle the accommodation problem, two methodological contributions based on the statistical archetypal analysis are put forward. An archetype in Statistics is an extreme observation that is obtained as a convex combination of other subjects of the sample \citep{Cutler1994}. The first of these methodologies was published in \cite{EpiVinAle}, whereas the second one has been submitted for publication \citep{Vinue2013Arch}. 

As far as we know, there is currently no reference in the literature related to Anthropometry or Ergonomics that provides the programming of the algorithms proposed. In addition, to the best of our knowledge, with the exception of modern human modelling tools like Jack and Ramsis, which are two of the most widely used tools by a broad range of industries \citep{Jack}, there are neither general software applications nor statistical packages available from the Internet, to tackle the definition of an efficient sizing system or the accommodation problem. In this context, a new \proglang{R} package \citep{R} called \pkg{Anthropometry} is introduced in this paper, which brings together all the abovementioned methodologies. All of them were applied to the anthropometric study of the Spanish female population and to the 1967 United States Air Force (USAF) survey. \pkg{Anthropometry} includes several data files related to both anthropometric databases. All the statistical methodologies, anthropometric databases and this R package were announced in the author's PhD thesis \citep{Tesis}, which will soon be freely available on two Spanish institutional open archives.

The outline of the paper is as follows: Section \ref{data} describes all the data files included in \pkg{Anthropometry}. Section \ref{methods} gives a brief exposition of each statistical technique suggested and Section \ref{Rcode} presents how they are implemented in this package. In Section \ref{examples}, some examples of their application are shown. Finally, concluding remarks are in Section \ref{conclusions}.


\section{Data}\label{data}

\subsection{Spanish anthropometric survey}

The Spanish National Institute of Consumer Affairs (INC according to its Spanish acronym) of the Spanish Ministry of Health and Consumer Affairs commissioned a 3D anthropometric study of the Spanish female population in 2006, after signing a commitment with the main Spanish companies in the apparel industry. The Spanish National Research Council (CSIC in Spanish) planned and developed the design of experiments, the Complutense University of Madrid was responsible for providing advice on Anthropometry and the study itself was conducted by the Biomechanics Institute of Valencia \citep{Alemany2010}. The target sample was made up of 10415 women grouped into 10 age groups ranging from 12 to 70 years, randomly chosen from the official Postcode Address File.

As illustrative data of the whole Spanish survey, \pkg{Anthropometry} contains a database called \code{dataDemo}, made up of a sample of 600 Spanish women and their measurements for five anthropometric variables: bust, chest, waist and hip circumferences and neck to ground length. These variables are chosen because three main reasons: recommendations of experts, they are commonly used in the literature and they appear in the European standard to sizing system. Size designation of clothes. Part 2: Primary and secondary dimensions \citep{NormaUNE2}.

This data set will be used by \emph{trimowa}, \emph{TDDclust} and \emph{hipamAnthropom}. As said, the women shape is represented by a set of landmarks, specifically 66 points. A data file called \code{landmarks} contains the configuration matrix of landmarks for each of the 600 women. The \emph{kmeansProcrustes} methodology will need this data file.

As also noted above, a 3D binary image of each woman's trunk is available. Hence, the dissimilarity between trunk forms can be computed and a distance matrix between women can be built. The distance matrix used in \cite{Vinue2013Arch} is included in \pkg{Anthropometry} and it is called \code{cMDSwomen}. 

\subsection{USAF survey}

This database saves the information provided by the 1967 United States Air Force (USAF) Survey. It can be  downloaded from \href{http://www.dtic.mil/dtic/}{http://www.dtic.mil/dtic/}. This survey was conducted during 1967 by the Anthropology Branch of the Aerospace Medical Research Laboratory (Ohio). A sample of 2420 subjects of the Air Force personnel, between 21 and 50 years of age, was measured at 17 Air Force bases across the United States of America. A total of 202 variables were collected. The dataset associated with the USAF survey is available in \code{dataUSAF}. In the methodologies related to the archetypal analysis, six anthropometric variables from the total of 202 will be selected. They are the same selected in \cite{Zehner1983} and are called cockpit dimensions because they are critical in order to design an aircraft cockpit. 

\subsection{Geometric figures}

In the \emph{kmeansProcrustes} approach, a numerical simulation with controlled data is performed to show the utility of our methodology. The controlled data are two geometric figures, a cube and a parallelepiped, made up of 8 and 34 landmarks. These configurations are saved in four files called \code{cube8}, \code{cube34}, \code{parallelepiped8} and \code{parallelepiped34}, respectively.


\section{Statistical methodologies}\label{methods}

In Section \ref{clust}, the \emph{trimowa}, \emph{CCbiclustAnthropo}, \emph{TDDclust} and \emph{hipamAnthropom} are described. Section \ref{ssa} focuses on the \emph{kmeansProcrustes} methodology. Section \ref{Arch} provides an exposition of the methodologies based on the archetypal analysis.

As a practical guidance, the way of proceeding of the clustering-based approaches has been the following: the data matrix was segmented using a primary control dimension (bust circumference in the case of \emph{trimowa}, \emph{hipamAnthropom}, \emph{kmeansProcrustes} and \emph{TDDclust}, and waist circumference in the case of \emph{CCbiclustAnthropo}, according to the classes suggested in the European standard to sizing system. Size designation of clothes. Part 3: Measurements and intervals \citep{NormaUNE3}). Then, a further segmentation using other secondary control anthropometric variables is carried out. In this way, the first segmentation provides a first easy input to choose the size, while the resulting clusters (subgroups) for each bust (or waist) and other anthropometric measurements optimize sizing. 
                                                                                                                                                              
Regarding the methodologies using the archetypal analysis, the steps are as follows: first, depending on the problem, to standardize or not the data. Then, selecting an accommodation subsample for obtaining the archetypal individuals as the third and last step.                                                                                                                          
                                                                                                                                                        

\subsection{Antropometric dimensions based clustering}\label{clust}

\vspace*{0.2cm}
%\subsubsection{Trimowa}
{\bf Trimowa}

The aim of a sizing system is to divide a varied population into groups using some key body dimensions \citep{libroAshdown,Chung2007}. Three types of approaches can be distinguished for creating a sizing system: traditional step-wise sizing, multivariate methods and optimization methods. Traditional methods are not useful because they use bivariate distributions to define a sizing chart and they do not consider the variability of other relevant anthropometric dimensions. Recently, more sophisticated statistical methods have been developed, especially using Principal Component Analysis (PCA) and clustering \citep{Gupta2004,Hsu09,Luximon2012,Hsu2009Apparel,Chung2007,Zheng2007,Bagherzadeh2010}. Peter Tryfos was the first one that suggested an optimization method \citep{Tryfos1986}. Later on, McCulloch et al. \citep{McCullochPaalAshdown98} modified the Tryfos' approach. 

The first clustering methodology proposed, called \emph{trimowa}, is closed to that one developed in \cite{McCullochPaalAshdown98}. However, there are two main differences. First, when searching for $k$ prototypes, a more statistical approach is assumed. To be quite specific, a trimmed version of the Partioning Around Medoids (or $k$-medoids) clustering algorithm is used. The trimming procedure allows us to remove outlier observations \citep{GE2008,Garcia-Escudero:2003:TTE}. Second, the dissimilarity measure defined in \cite{McCullochPaalAshdown98} is modified by using an OWA (Ordered Weighted Average) operator to consider the user morphology. Our approach allows us the derivation of more realistic prototypes (medoids) because they correspond to real women of the database and the selection of individual discommodities. In addition, the use of OWA operators has resulted in a more realistic dissimilarity measure between individuals and prototypes. This approach was published in \cite{Ibanez2012} and it is implemented in the \code{trimowa} function.

We learned from this situation that there is an ongoing search for advanced statistical approaches that can deliver practical solution solutions to the definition of central people and optimal size groups. Consequently, we have come across in the literature two different statistical strategies and we have aimed to discuss their potential usefulness in the definition of an efficient clothing sizing system. These approaches are based on Biclustering and Data Depth and will be summarized in the next two sections.

\vspace*{0.5cm}

%\subsubsection{CCbiclustAnthropo}
{\bf CCbiclustAnthropo}

In the analysis of gene expression data, conventional clustering are limited to find local expression patterns. Gene data are organized in a data matrix where rows correspond to genes and columns to experimental samples (conditions). The goal is to find submatrices, i.e., subgroups of genes and subgroups of conditions, where the genes exhibit high correlation for every condition \citep{Madeira2004}. Biclustering is a novel clustering approach developed with this goal in mind. This technique consists in simultaneous partitioning of the set of rows and the set of columns into subsets. 

In a traditional row cluster, each row is defined using all the columns of the data matrix. Something analogue would occur with a column cluster. However, regarding biclustering, each row in a bicluster is defined using only a subset of columns and vice versa. Therefore, clustering derives a global model but biclustering defines a local one. This interesting property made us think that maybe biclustering could be useful to derive efficient size groups, since they would be only defined for the most relevant anthropometric dimensions that describe a body in the detail necessary to design a garment that fit well.

Recently, a large number of biclustering methods have been developed. Some of them are implemented in different sources, including R. Up to now, the most complete R package for biclustering is \pkg{biclust} \citep{Kaiser2008,biclust}. The utility of the approaches included in \pkg{biclust} to deal with anthropometric data was investigated in \cite{Vinue2012}. Among the conclusions reached, the most important was concerned with the possibility to consider the Cheng \& Church biclustering algorithm \citep{Cheng2000} (from now on, CC) as a potential statistical approach to be used for defining size groups. Specifically, in \cite{Vinue2012} an algorithm to find size groups (biclusters) and disaccommodated women with CC was set out. This methodology is called \emph{CCbiclustAnthropo} and it is implemented in the \code{CCbiclustAnthropo} function. 

Designing lower body garments depends not only on the waist circumference (the principal dimension in this case), but also on other secondary control dimensions (for upper body garments the bust circumference is usually needed only). Biclustering produces subgroups of objects that are similar in one subgroup of variables and different in the remaining variables. Therefore, it seems more interesting to use a biclustering algorithm with a set of lower body dimensions. For that purpose, all the body variables related to the lower body part included in the Spanish anthropometric survey were chosen (there were 36). As promising results, an efficient partition into different biclusters was obtained. All individuals in the same bicluster can wear a garment designed for the specifical body dimensions (waist and other variables) which were the most relevant for defining the group. Each group is represented by the median woman. The CC algorithm is nonexhaustive, i.e, some rows (and columns) do not belong to any bicluster. This property can be used to fix a proportion of no accommodated sample.

The major interest of this approach was descriptive and exploratory and the important point to note here is that \code{CCbiclustAnthropo} cannot be used with \code{dataDemo}, since this data file does not contain variables related to the lower body part in addition to waist and hip. However, this function is included in the package in the hope that it could be helpful or useful for other researchers. All theoretical and practical details are given in \cite{Tesis} and \cite{Vinue2012}.

\vspace*{0.5cm}

%\subsubsection{TDDclust}
{\bf TDDclust}

The statistical concept of data depth is other general framework for descriptive and inferential analysis of numerical data in a certain number of dimensions. In essence, the notion of data depth is a generalization of the median to data in higher-dimensions. A depth function measures the degree of centrality of a point regarding a probability distribution or a data set. Highest depth values correspond to central points and lowest depth values correspond to tail points \citep{Liu1999,Zuo2000}. Therefore, the depth paradigm is other very interesting strategy to identify central prototypes.

The development of clustering and classification methods using data depth measures has received increasing attention  over the last few years \citep{Duttaetal12,Langeetal12,Lopez2010,Ding2007}. The most relevant contribution to this field has been made by Rebecka J\"ornsten in \cite{Jornsten2004} (see also \cite{Jornsten2002} and \cite{Pan2004}). She introduced two clustering and classification methods (\emph{DDclust} and \emph{DDclass}, respectively) based on $L_1$ data depth (see \cite{Vardi2000}). The \emph{DDclust} method is proposed to solve the problem of minimizing the sum of $L_1$-distances from the observations to the nearest cluster representatives. The $L_1$ data depth is the amount of probability mass needed at a point $z$ to make $z$ the multivariate $L_1$-median (a robust representative) of the data cluster.

An extension of \emph{DDclust} by incorporating a trimmed procedure is introduced, aimed at segmenting the data into efficient size groups, using central (the deepest) people. In what follows, this methodology will be called \emph{TDDclust} and it can be used within \pkg{Anthropometry} by using a function with the same name. All details about \emph{TDDclust} are described in \cite{Tesis}.


\vspace*{0.5cm}

%\subsubsection{hipamAnthropom}
{\bf hipamAnthropom}

Representative fit models are important for defining a meaningful sizing system. However, there is no agreement among apparel manufacturers and almost every company employs a different fit model. Companies try to improve the quality of garment fit by scanning their fit models and deriving dress forms from the scans \citep{libroAshdown,Song2010}. A fit model's measurements correspond to the commercial specifications established by each company to achieve the company's fit \citep{Loker2005,Workman2000,Workman1991}. Beyond merely wearing the garment for inspection, a fit model provides objective feedback about fit, movement or comfort of a garment in place of the consumer. 

In order to provide new insights about this problem, the \emph{hipamAnthropom} methodology is proposed. It consists of two classification algorithms based on the HIerarchical Partitioning Around Medoids (HIPAM) clustering method presented in \cite{Wit2004}, modified to deal with anthropometric data. This procedure was published in \cite{Vinue2013}. The dissimilarity measure defined in \cite{McCullochPaalAshdown98} and a different method to obtain a classification tree \citep{Irigoien2008} were incorporated. One algorithm was called $HIPAM_{MO}$ and the other one, $HIPAM_{IMO}$. The outputs of both of them include a set of central representative subjects or medoids taken from the original data set which constitute our fit models. In addition, they can detect outliers.  This methodology is available in the \code{hipamAnthropom} function.


\subsection{Statistical shape analysis: kmeansProcrustes}\label{ssa}

The clustering methodologies explained in Section \ref{clust} use a set of control anthropometric variables as the basis for a different type of sizing system in which people are grouped in a size group based on a full range of measurements. Consequently, clustering is done in the Euclidean space. The shape of the women recruited into the Spanish anthropometric survey is represented by a set of correspondence points called landmarks. Taking advantage of this fact, we have adapted the $k$-means clustering algorithm to the field of statistical shape analysis, to define size groups of women according to their body shapes \citep{Vinue2013ssa}. The representative of each size group is the mean woman.

The $k$-means method is based on the fact that the sample mean is the value that minimizes the Euclidean distance from each point, to the centroid of the cluster to which it belongs. In order to adapt $k$-means to shape analysis, it is sufficient merely to replace the sample mean and the Euclidean distance by the Procrustes mean and the Procrustes distance, which are basic concepts of the shape analysis \citep{DrydenMardia1998}. Thus, $k$-means can be used to cluster objects based on landmarks. Several attempts have been made in this regard, each one adapting a different version of the $k$-means. Amaral et al. in \citet{Amaral2010} adapted the Hartigan-Wong (H-W) version of $k$-means, while V. Georgescu in \citet{Georgescu2009} used an algorithm in some aspects similar to the Lloyd version of $k$-means. 

The main difference between Lloyd and H-W is that H-W proceeds point by point. Therefore, it requires the mean to be updated many more times than Lloyd. Unlike the sample mean in the Euclidean space, the calculus of the Procrustes mean needs much more time. Consequently, H-W should have a high computational burden in the shape space, losing efficiency, especially with a large sample size. In order to confirm empirically these hypothesis, we adapted both H-W and original Lloyd versions of $k$-means to the field of shape analysis and we demonstrated, by means of a simulation study, that the original Lloyd version of $k$-means is more efficient to clustering shapes than the H-W version \citep{Vinue2013ssa}.

The function to use the Lloyd version of $k$-means adapted to shape analysis (what we called \emph{kmeansProcrustes}) is \code{LloydShapes}. In addition, the function to use the Hartigan-Wong version of $k$-means adapted to shape analysis is \code{HartiganShapes}. In addition, a trimmed version of \emph{kmeansProcrustes} can be executed with \code{trimmedLloydShapes}.




\subsection{Archetypal Analysis}\label{Arch}

In ergonomic related problems, where the goal is to create more efficient people-machine interfaces, a small set of extreme cases (boundary cases), called human models, is searched. Designing for extreme individuals is appropriate where some limiting factor can define either a minimum or maximum value which will accommodate the population. The basic principle is that accommodating boundary cases will be sufficient to accommodate the whole population.

For too long, the classical solution to select this small group of boundary models was based on the use of percentiles. However, percentiles are a kind of univariate descriptive statistic, so they are suitable only for univariate accommodation and should not be used in designs that involve two or more dimensions. In addition, they are not additive \citep{Zehner1983,Robinette1981,Moroney1972}. Today, the alternative commonly used for the multivariate accomodation problem is based on PCA \citep{Friess2003,Hudson1998,Robinson1992,Bittner1987}. However, it is known that the PCA approach presents some drawbacks \citep{Friess2005}. In \cite{EpiVinAle}, a different statistical approach for determining multivariate limits was put forward: the archetypal analysis \citep{Cutler1994}, and its advantages regarding to PCA were demonstrated. The function that allows us to reproduce the results discussed in \cite{EpiVinAle} is \code{archetypesBoundary}.

Archetypes computed by archetypal analysis are a convex combination of the sampled individuals, but they are not necessarily real observations. In some problems, it is crucial that the archetypes are real subjects, observations of the sample, and not fictitious. To that end, we have proposed a new archetypal concept: the archetypoid, which correspond to specific individuals and each observation of the data set can be represented as a mixture of these archetypoids \citep{Vinue2013Arch}. We have developed an efficient computational algorithm based on PAM to compute archetypoids (called archetypoid algorithm), we have analyzed some of their theoretical properties, we have explained how they can be obtained when only dissimilarities between observations are known (features are unavailable) and we have demonstrated some of their advantages regarding classical archetypes. The \code{stepArchetypoids} function calls the \code{archetypoids} function to run the archetypoid algorithm repeatedly.

\section{The Anthropometry R package}\label{Rcode}

First, the package must be loaded into \proglang{R}:

<<paquete,eval=FALSE>>=
library(Anthropometry)
@

Next we briefly describe the \proglang{R} functions to execute each one of the methodologies introduced in Section \ref{methods}.

\subsection{Antropometric dimensions based clustering}

A key element of two of the aforementioned clustering methodologies, \emph{trimowa} and \emph{hipamAnthropom}, is the dissimilarity function used. It is based on the dissimilarity measure proposed in \cite{McCullochPaalAshdown98}. The code to compute the dissimilarities with \pkg{Anthropometry} is written in \proglang{C} and it is exported from the NAMESPACE file. 

Both clustering methodologies incorporate the calculus of the dissimilarity matrix within their main functions,  \code{trimowa} and \code{hipamAnthropom}, respectively.


\textbf{\code{trimowa} function}

<<functiontrimowa,eval=FALSE,tidy=FALSE>>=
trimowa(x,w,K,alpha,niter,Ksteps,ahVect=c(23,28,20,25,25))
@

Its arguments are the following:

\begin{itemize}

\item x: Data frame. In our approach, this is each one of the subframes originated after segmenting the whole anthropometric Spanish survey in twelve bust segments, according to the European standard to sizing system. Size designation of clothes. Part 3: Measurements and intervals. Each row corresponds to an observation, and each column corresponds to a variable. All variables are numeric.

\item w: The aggregation weights of the OWA operators. They are computed with the \code{WeightsMixtureUB}.

\item K: Number of clusters.

\item alpha: Proportion of trimmed sample.

\item niter: Number of random initializations.

\item Ksteps: Steps per initialization.

\item ahVect: Constants that define the \emph{ah} slopes of the distance function in \code{GetDistMatrix}. Given the five variables considered, this vector is c(23,28,20,25,25). This vector would be other according to the variables considered.

\end{itemize}

\vspace*{0.5cm}

\textbf{\code{TDDclust} function}

<<functionTDDclust,eval=FALSE,tidy=FALSE>>=
TDDclust(x,K,lambda,Th,A,T0,alpha,lplot,Trimm,data1)
@

Its arguments are the following:

\begin{itemize}

\item x: Data frame. Each row corresponds to an observation, and each column corresponds to a variable. All variables must be numeric.

\item K: Number of clusters. 

\item lambda: Tuning parameter that controls the influence the data depth has over the clustering, see \cite{Jornsten2004}.

\item Th: Threshold for observations to be relocated, usually set to 0.

\item A: Number of iterations. 

\item T0: Simulated annealing parameter. It is the current temperature in the simulated annealing procedure.

\item alpha: Simulated annealing parameter. It is the decay rate, default 0.9. 

\item lplot: Tracking convergence, default 0.

\item Trimm: Proportion of no accommodated sample.

\item data1: The same data frame as \emph{x}, used to incorporate the trimmed observations to the rest of them for the next iteration. 

\end{itemize}


\vspace*{0.5cm}

\textbf{\code{hipamAnthropom} function}

<<functionhipam,eval=FALSE,tidy=FALSE>>=
hipamAnthropom(x,asw.tol=0,maxsplit=5,local.const=NULL,orness=0.7,
               type,ahVect=c(23,28,20,25,25),...)
@

Its arguments are the following:

\begin{itemize}

\item x: Data frame. In our approach, this is each one of the subframes originated after segmenting the whole anthropometric Spanish survey in twelve bust segments, according to the European standard to sizing system. Size designation of clothes. Part 3: Measurements and intervals. Each row corresponds to an observation, and each column corresponds to a variable. All variables are numeric.

\item asw.tol: If this value is given, a tolerance or penalty can be introduced (asw.tol > 0 or asw.tol < 0, respectively) in the branch splitting procedure. Default value (0) is maintained. See \cite[p.154]{Wit2004} for more details.

\item maxsplit: The maximum number of clusters that any cluster can be divided when searching for the best clustering.

\item local.const: If this value is given (meaningful values are those between -1 and 1), a proposed partition is accepted only if the associated asw is greater than this constant. Default option for this argument is maintained, that is to say, this value is ignored. See \cite[p.154]{Wit2004} for more details.

\item orness: Quantity to measure the degree to which the aggregation is like a min or max operation. See \code{WeightsMixtureUB} and \code{GetDistMatrix}.

\item type: Type of HIPAM algorithm to be used. The possible options are 'MO' (for $HIPAM_{MO}$) and 'IMO' (for $HIPAM_{IMO}$).

\item ahVect: Constants that define the \emph{ah} slopes of the distance function in \code{GetDistMatrix}. Given the five variables considered, this vector is c(23,28,20,25,25). This vector would be other according to the variables considered.

\item ...: Other arguments that may be supplied to the internal functions of the HIPAM algorithms.

\end{itemize}



\subsection{Statistical shape analysis}

\vspace*{0.5cm}

\textbf{\code{LloydShapes} function}

<<functionLloyd,eval=FALSE,tidy=FALSE>>=
LloydShapes(dg,K,Nsteps=10,niter=10,stopCr=0.0001,simul,print)
@

Its arguments are the following:

\begin{itemize}

\item dg: Array with the 3D landmarks of the sample objects. Each row corresponds to an observation, and each column corresponds to a dimension (x,y,z).

\item K: Number of clusters.

\item Nsteps: Number of steps per initialization. Default value is 10.

\item niter: Number of random initializations. Default value is 10.

\item stopCr: Relative stopping criteria. Default value is 0.0001.

\item simul: A logical value. If TRUE, this function is used for a simulation study.

\item print: Logical value. If TRUE, some messages associated with the running process are displayed.

\end{itemize}


\vspace*{0.5cm}

\textbf{\code{HartiganShapes} function}

<<functionHW,eval=FALSE,tidy=FALSE>>=
HartiganShapes(dg,K,Nsteps=10,niter=10,stopCr=0.0001,simul,initLl,
               initials,print)
@

Its arguments are the following:

\begin{itemize}

\item dg: Array with the 3D landmarks of the sample objects. Each row corresponds to an observation, and each column corresponds to a dimension (x,y,z).

\item K: Number of clusters.

\item Nsteps: Number of steps per initialization. Default value is 10.

\item niter: Number of random initializations. Default value is 10.

\item stopCr: Relative stopping criteria. Default value is 0.0001.

\item simul: A logical value. If TRUE, this function is used for a simulation study.

\item initLl: Logical value. If TRUE, see next argument \emph{initials}. If FALSE, they are new random initial values.

\item initials: If \emph{initLl=TRUE}, they are the same random initial values used in each iteration of \code{LloydShapes}. If \emph{initLl=FALSE} this argument must be passed simply as an empty vector.

\item print: Logical value. If TRUE, some messages associated with the running process are displayed.

\end{itemize}

\vspace*{0.5cm}

\textbf{\code{trimmedLloydShapes} function}

<<functionkmeansSSA,eval=FALSE,tidy=FALSE>>=
trimmedLloydShapes(dg,n,alpha,K,Nsteps=10,niter=10,
                   stopCr=0.0001,print)
@

Its arguments are the following:

\begin{itemize}

\item dg: Array with the 3D landmarks of the sample objects. Each row corresponds to an observation, and each column corresponds to a dimension (x,y,z).

\item n: Number of individuals.

\item alpha: Proportion of trimmed sample.

\item K: Number of clusters.

\item Nsteps: Number of steps per initialization. Default value is 10.

\item niter: Number of random initializations. Default value is 10.

\item stopCr: Relative stopping criteria. Default value is 0.0001.

\item print: Logical value. If TRUE, some messages associated with the running process are displayed.

\end{itemize}


\subsection{Archetypal analysis}

\vspace*{0.5cm}

\textbf{\code{archetypesBoundary} function}

<<archetUSAF,eval=FALSE,tidy=FALSE>>=
archetypesBoundary(data,numArchet,verbose,nrep)
@

\begin{itemize}

\item data: USAF 1967 database (see \code{dataUSAF}). Each row corresponds to an observation, and each column corresponds to a variable. All variables are numeric.

\item numArchet: Number of archetypes.

\item verbose: Logical value. If TRUE, some details of the execution progress are shown (this is the same argument as that of the \code{stepArchetypes} function of the \pkg{archetypes} \proglang{R} package \citep{Eugster2009}).

\item nrep: For each archetype run \code{archetypes} nrep times (this is the same argument as that of the \code{stepArchetypes} function of \pkg{archetypes}).

\end{itemize}


\vspace*{0.5cm}

\textbf{\code{archetypoids} function}

<<Arch,eval=FALSE,tidy=FALSE>>=
archetypoids(i,data,huge=200,step,init,ArchObj,nearest,sequ,aux)
@

\begin{itemize}

\item i: Number of archetypoids.

\item data: Data matrix. Each row corresponds to an observation and each column corresponds to an anthropometric variable. All variables are numeric.

\item huge: This is a penalization added to solve the convex least squares problems regarding the minimization problem to estimate archetypoids, see \cite{Eugster2009}. Default value is 200.

\item step: Logical value. If TRUE, the archetypoid algorithm is executed repeatedly within \code{stepArchetypoids}. Therefore, this function requires the next argument \emph{init} (but neither the \emph{ArchObj} nor the \emph{nearest} arguments) that specifies the initial vector of archetypoids, which has been already computed within \code{stepArchetypoids}. If FALSE, the archetypoid algorithm is executed once. In this case, the \emph{ArchObj} and \emph{nearest} arguments are required to compute the initial vector of archetypoids.

\item init: Initial vector of archetypoids for the BUILD phase of the archetypoid algorithm. It is computed within \code{stepArchetypoids}. See next \emph{nearest} argument to know how this vector is calculated.

\item ArchObj: The list returned by the \code{stepArchetypesMod} function. This function is a slight modification of the original \code{stepArchetypes} function of \pkg{archetypes} to apply the archetype algorithm to raw data. The \code{stepArchetypes} function standardizes the data by default and this option is not always desired. This list is needed to compute the nearest individuals to archetypes. Required when \emph{step=FALSE}.

\item nearest: Initial vector of archetypoids for the BUILD phase of the archetypoid algorithm. Required when \emph{step=FALSE}. This argument is a logical value: if TRUE (FALSE), the \emph{nearest} (\emph{which}) vector is calculated. Both vectors contain the nearest individuals to the archetypes returned by the \code{archetypes} function of \pkg{archetypes} (In \cite{Vinue2013Arch}, archetypes are computed after running the archetype algorithm twenty times). The \emph{nearest} vector is calculated by computing the Euclidean distance between the archetypes and the individuals and choosing the nearest. It is used in \cite{EpiVinAle}. The \emph{which} vector is calculated by identifying consecutively the individual with the maximum value of alpha for each archetype, until getting the number of archetypes defined. It is used in \cite{Eugster2012}. 

\item sequ: Logical value. It indicates whether a sequence of archetypoids (TRUE) or only a single number of them (FALSE) is computed. It is determined by the number of archetypes computed by means of \code{stepArchetypesMod}.

\item aux: If \emph{sequ}=FALSE, this value is equal to \emph{i}-1 since for a single number of archetypoids, the list associated with the archetype object only has one element.

\end{itemize}


\vspace*{0.5cm}

\textbf{\code{stepArchetypoids} function}

<<stepArch,eval=FALSE,tidy=FALSE>>=
stepArchetypoids(i,nearest,data,ArchObj)
@

\begin{itemize}

\item i: Number of archetypoids.

\item nearest: Initial vector of archetypoids for the BUILD phase of the archetypoid algorithm. This argument is a logical value: if TRUE (FALSE), the \emph{nearest} (\emph{which}) vector is calculated. Both vectors contain the nearest individuals to the archetypes returned by the \code{archetypes} function of \pkg{archetypes} (In \cite{Vinue2013Arch}, archetypes are computed after running the archetype algorithm twenty times). The \emph{nearest} vector is calculated by computing the Euclidean distance between the archetypes and the individuals and choosing the nearest. It is used in \cite{EpiVinAle}. The \emph{which} vector is calculated by identifying consecutively the individual with the maximum value of alpha for each archetype, until getting the number of archetypes defined. It is used in \cite{Eugster2012}. 

\item data: Data matrix. Each row corresponds to an observation and each column corresponds to an anthropometric variable. All variables are numeric.

\item ArchObj: The list returned by the \code{stepArchetypesMod} function. This function is a slight modification of the original \code{stepArchetypes} function of \pkg{archetypes} to apply the archetype algorithm to raw data. The \code{stepArchetypes} function standardizes the data by default and this option is not always desired. This list is needed to compute the nearest individuals to archetypes. 

\end{itemize}


\vspace*{0.5cm}

\textbf{\code{stepArchetypesMod} function}

<<stepArchMod,eval=FALSE,tidy=FALSE>>=
stepArchetypesMod(data,k,nrep=3,verbose=TRUE)
@

\begin{itemize}

\item data: Data to obtain archetypes.

\item k: Number of archetypes to compute, from 1 to \emph{k}.

\item nrep: For each \emph{k}, run \code{archetypes} \emph{nrep} times.

\item verbose: If TRUE, the progress during exection is shown.

\end{itemize}


\section{Examples}\label{examples}

\subsection[Clustering]{Antropometric dimensions based clustering}

The following code executes the \code{trimowa} methodology. A similar code was used to obtain the results described in \cite{Ibanez2012}. In the next example, the number of random initializations is 10, with three steps per initialization. Figure~\ref{trimowaExample} shows the scatter plots of bust circumference against neck to ground with the three medoids obtained for each bust class without (left) and with (right) the prototypes defined by the European standard. Note that any other number of clusters could be also obtained.  

<<trimowa,eval=FALSE,tidy=FALSE>>=
#Data loading:
dataDef <- dataDemo
num.variables <- dim(dataDef)[2]
bust <- dataDef$bust

#Aggregation weights calculation:
orness <- 0.7
w <- WeightsMixtureUB(orness,num.variables)

#Bust classes definition according to the European standard:
bustCirc_4 <- seq(74,102,4)  
bustCirc_6 <- seq(107,131,6)  
bustCirc <- c(bustCirc_4,bustCirc_6) 

#Trimowa parameters:
nsizes <- length(bustCirc)
K <- 3 ; alpha <- 0.01 ; niter <- 10 ; Ksteps <- 7
ahVect <- c(23,28,20,25,25)

#For reproducing results, seed for randomness:
set.seed(2014)
res_trimowa <- list()
for (i in 1 : (nsizes-1)){ 
data = dataDef[(bust >= bustCirc[i]) & (bust < bustCirc[i + 1]), ]   
res_trimowa[[i]] <- trimowa(data,w,K,alpha,niter,Ksteps,ahVect=ahVect)
}

#Medoids provided for each bust class:
medoids <- list()
for (i in 1 : (nsizes-1)){ 
medoids[[i]] <- res_trimowa[[i]]$meds 
} 

#Plotting arguments:
bustVariable <- "bust"
xlim <- c(70,150)
color <- c("black", "red", "green", "blue", "cyan", "brown", "gray", 
           "deeppink3", "orange", "springgreen4", "khaki3", "steelblue1")

variable <- "necktoground"
ylim = c(110,160) #see range(dataDef[,variable])
title <- "Medoids \n bust vs neck to ground"

#Figure 1 (left and right):
plotMedoids(dataDef,medoids,nsizes,bustVariable,variable,color,
            xlim,ylim,title,FALSE)
plotMedoids(dataDef,medoids,nsizes,bustVariable,variable,color,
            xlim,ylim,title,TRUE)
@

\begin{figure}[ht]
\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{c}
\includegraphics[width=\textwidth]{trimowa1.pdf}
\end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{trimowa2.pdf}
\end{minipage}
\caption{Bust vs neck to ground, jointly with our medoids (left) and the prototypes defined by the European standard (right).}
\label{trimowaExample}
\end{figure}

In addition, the following sentences illustrate how to use the \code{hipamAnthropom} methodology. Specifically, some results returned by the $HIPAM_{IMO}$ algorithm are shown. Figure~\ref{hipamExample} displays the medoids (left) and the outlier women (right) corresponding to each bust size. 

We emphasize that \code{dataDemo} is divided into twelve bust segments here as well. Thus, each segment has a small sample size. This might explain that this algorithm (and also $HIPAM_{MO}$) does not find big homogeneous clusters and therefore, identifies a lot of women as outliers in each class for this case. One of the features of the HIPAM algorithm is that it is a very sensitive algorithm to identify outliers. A broad discussion, analysis and thoughts of the anthropometric meanining of these outliers is given in \cite{Vinue2013} (including its supplementary material).

<<hipam,eval=FALSE,tidy=FALSE>>=
#Data loading:
dataDef <- dataDemo
bust <- dataDef$bust

#Bust classes definition according to the European standard:
bustCirc_4 <- seq(74,102,4)  
bustCirc_6 <- seq(107,131,6)  
bustCirc <- c(bustCirc_4,bustCirc_6) 

#HipamAnthropom parameters:
nsizes <- length(bustCirc)
maxsplit <- 5 ; orness <- 0.7 ; type <- "IMO" 
#type <- "MO" for HIPAM_{MO}
ahVect <- c(23, 28, 20, 25, 25)

#For reproducing results, seed for randomness:
set.seed(2013)
hip <- list()
for(i in 1 : (nsizes - 1)){
data =  dataDef[(bust >= bustCirc[i]) & (bust < bustCirc[i + 1]), ]   
d <- as.matrix(data)
hip[[i]] <- hipamAnthropom(d,maxsplit=maxsplit,orness=orness,
                           type=type,ahVect=ahVect) 
}   

#hipamBigGroups is a function of Anthropometry that returns the medoids 
#of the clusters with more than 2 elements:
list.meds <- lapply(1:(nsizes - 1), FUN = hipamBigGroups, hip)
#outlierHipam is a function of Anthropometry that returns the individuals 
#of the clusters with 1 or 2 elements (outliers):
list_outl1_2 <- sapply(1 : (nsizes - 1), FUN = outlierHipam, hip)

#Plotting arguments:
bustVariable <- "bust"
xlim <- c(70,150)
color <- c("black", "red", "green", "blue", "cyan", "brown", "gray", 
           "deeppink3", "orange", "springgreen4", "khaki3", "steelblue1")

variable <- "hip"
ylim <- c(80,160)
title <- "Medoids HIPAM_IMO \n bust vs hip"
title_outl <- "Outlier women HIPAM_IMO \n bust vs hip"

#Figure 2 (left and right):
plotMedoids(dataDef,list.meds,nsizes,bustVariable,variable,color,
            xlim,ylim,title,FALSE)
plotTrimmOutl(dataDef,list_outl1_2,nsizes,bustVariable,variable,color,
              xlim,ylim,title_outl)
@

\begin{figure}[ht]
\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{c}
\includegraphics[width=\textwidth]{hipam1.pdf}
\end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{hipam2.pdf}
\end{minipage}
\caption{Bust vs hip with the medoids (left) and with the outliers (right) obtained using $HIPAM_{IMO}$.}
\label{hipamExample}
\end{figure}

Next, a basic example of the \emph{TDDclust} methodology is shown.


<<TDDclust,eval=FALSE,tidy=FALSE>>=
#In the interests of simplicity of the computation involved
#only a small sample (the first 50 individuals) is selected:
dataDef <- dataDemo[1:50,c(2,3,5)] #Neck to ground, waist and bust variables.
data1 <- dataDemo[1:50,c(2,3,5)]

#TDDclust parameters:
K=3     ; lambda=0.5    ; Th=0
A=5     ; T0=0          ; alpha=.9   
lplot=0 ; percTrimm=0.1     

Dout <- TDDclust(x=dataDef,K=K,lambda=lambda,Th=Th,A=A,T0=T0,alpha=alpha,
                 lplot=lplot,Trimm=percTrimm,data1=data1) 

#Clustering results:
table(Dout$NN[1,]) 
#Final value of the optimal partition:
Dout$Cost
#Iteration in which the optimal partition was found:
Dout$klBest
#Trimmed observations:
Dout$indivTrimmed
@



\subsection{Statistical shape analysis}

Next, the results obtained with the trimmed \emph{kmeansProcrustes} by using five iterations, five number of steps per initialization and a relative stopping criteria of 0.0001 are presented. Figure~\ref{kmProcExample} displays the boxplots for neck to ground measurement for the three clusters calculated (left) and the projection on the plane xy of the registrated points and mean shape for cluster 1 (right).

<<ssa,eval=FALSE,tidy=FALSE>>=
landmarks1 <- na.exclude(landmarks)
num.points <- (dim(landmarks1)[2]) / 3
#In the interests of simplicity of the computation involved
#only a small sample (the first 50 individuals) is selected:
landmarks2 <- landmarks1[1:50,]
n <- dim(landmarks2)[1]

#Array with the 3D landmarks of the sample objects:
dg <- array(0,dim = c(num.points,3,n))
for(k in 1:n){            
 for(l in 1:3){            
  dg[,l,k] <- as.matrix(as.vector(landmarks2[k,][seq(l,dim(landmarks2)[2]+
                        (l-1),by=3)]),ncol=1,byrow=T)
 }
}

#kmeansProcrustes parameters:
K <- 3 ; alpha <- 0.01 ; Nsteps <- 5 ; niter <- 5 ; stopCr <- 0.0001
#For reproducing results, seed for randomness:
set.seed(2013)
res <- trimmedLloydShapes(dg,n,alpha,K,Nsteps,niter,stopCr,TRUE)

#Numerical and graphical results:
asig <- res$asig #table(asig) shows the clustering results.
copt <- res$copt #optimal centers.

#To identify the trimmed individuals of the optimal iteration:
iter_opt <- res$trimmsIter[length(res$trimmsIter)]
trimm <- res$trimmWomen[[iter_opt]][[res$betterNstep]]

#Generalised Procrustes analysis into each cluster:
out_proc <- list()
for(h in 1 : K){
 out_proc[[h]] = shapes::procGPA(dg[, , asig == h], distances = T, 
                                 pcaoutput = T)
}

data <- dataDemo[1:50,]
data <- data[-trimm,]
#Figure 3 (left):
boxplot(data$necktoground ~ as.factor(asig), main = "Neck to ground")
#Figure 3 (right):
plotshapes(out_proc[[1]]$rotated)
points(copt[,,1], col = 2)
legend("topleft", c("Registrated data", "Mean shape"), pch = 1, 
       col = 1:2, text.col = 1:2)
title("Procrustes registrated data for cluster 1 \n 
      with its mean shape superimposed", sub = "Plane xy")
@


\begin{figure}[ht]
\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{c}
\includegraphics[width=\textwidth]{kmProc1.pdf}
\end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\textwidth]{kmProc2.pdf}
\end{minipage}
\caption{Boxplots for the neck to ground measurement for three clusters (left) and projection on the plane xy of the registrated points and mean shape for cluster 1 (right). Results provided by trimmed \emph{kmeansProcrustes}.}
\label{kmProcExample}
\end{figure}

\subsection{Archetypal analysis}

Three archetypoids for \code{dataUSAF} are calculated with the following code. Figure~\ref{AAExample} shows the percentiles of three archetypoids, beginning with \emph{nearest} (left) and with \emph{which} (right).

<<AA,eval=FALSE,tidy=FALSE>>=
#Cockpit design problem:
#In the interests of simplicity of the computation involved
#only a small sample (the first 50 individuals) is selected:
m <- dataUSAF[1:50,]
#Variable selection (cockpit dimensions):
sel <- c(48,40,39,33,34,36)
#Changing to inches: 
mpulg <- m[,sel] / (10 * 2.54)

#Data preprocessing:
preproc <- accommodation(mpulg,TRUE,0.95,TRUE)

#For reproducing results, seed for randomness:
set.seed(2010) 
#Run archetype algorithm repeatedly from 1 to numArch archetypes:
numArch <- 10 ; nrep <- 20
lass <- stepArchetypesMod(data=preproc$data,k=1:numArch,verbose=FALSE,
                          nrep=nrep)  
screeplot(lass)

i <- 3 #number of archetypoids to compute.
res <- archetypoids(i,preproc$data,huge=200,step=FALSE,ArchObj=lass,
                    nearest=TRUE,sequ=TRUE)
res_which <- archetypoids(i,preproc$data,huge=200,step=FALSE,ArchObj=lass,
                          nearest=FALSE,sequ=TRUE)

aux <- res$archet
aux_wh <- res_which$archet
#In this case, the nearest and which archetypoids match
#(although the nearest and which archetypes do not),
#so it's enought to represent a single percentiles plot:
  
percs <- list()
#In case the nearest and which archetypoids don't match:
#percs_wh <- list()
for(j in 1:length(aux)){
 percs[[j]] <- sapply(1:dim(preproc$data)[2],compPerc,aux[j],preproc$data,0)
 #percs_wh[[j]] <- sapply(1:dim(preproc$data)[2],compPerc,aux_wh[j],
                        #preproc$data,0)
}
m <- matrix(unlist(percs),nrow=6,ncol=length(percs),byrow=F)
#m1 <- matrix(unlist(percs_wh),nrow=6,ncol=length(percs_wh),byrow=F)

#Figure 4:
barplot(m,beside=TRUE, 
        main = paste(i, " archetypoids", sep = ""), 
        ylim=c(0,100), ylab="Percentile")
@


\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{AA.pdf}
\caption{Percentiles of three archetypoids, beginning with \emph{nearest} and with \emph{which} for \code{dataUSAF}. In this case, the \emph{nearest} and \emph{which} archetypoids coincide.}\label{AAExample}
\end{figure}



\section{Conclusions}\label{conclusions}

New three-dimensional whole-body scanners have drastically reduced the cost and duration of the measurement process. These types of systems, in which the human body is digitally scanned and the resulting data converted into exact measurements, make it possible to obtain accurate, reproducible and updated anthropometric data. These databases constitute very valuable information to effectively design better-fitting clothing and workstations, to understand the body shape of the population and to reduce the design process cycle. Therefore, rigorous statistical methodologies and software applications must be developed to make the most of them. 

This paper introduces a new \proglang{R} package called \pkg{Anthropometry} that brings together different statistical methodologies concerning clustering, the statistical concept of data depth, the statistical shape analysis and the archetypal analysis, especially developed to deal with anthropometric data. They use the data obtained from a 3D anthropometric survey of the Spanish female population and from the USAF survey. Procedures related to clustering, data dept and shape analysis are aimed at defining optimal clothing size groups and central prototypes. The two approaches based on the archetypal analysis are useful for determining boundary human models which could be useful to improve industry practice in workspace design. 

The  \pkg{Anthropometry} \proglang{R} package is a positive contribution to tackle some statistical problems related to Ergonomics and Anthropometry. It provides a useful software tool to engineers and researchers in these fields so that they can analyze their anthropometric data in a comprehensive way.


\section*{Acknowledgments}

The author gratefully acknowledges the many helpful suggestions of I. Epifanio and G. Ayala. The author would also like to thank the Biomechanics Institute of Valencia for providing us with the Spanish anthropometric data set and the Spanish Ministry of Health and Consumer Affairs for having commissioned and coordinated the ``Anthropometric Study of the Female Population in Spain''. This paper has been partially supported by the following grants: TIN2009-14392-C02-01, TIN2009-14392-C02-02. 

\bibliography{Anthropometry}

\end{document} 








